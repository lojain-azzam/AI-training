{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lojain-azzam/AI-training/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAYNStAxDX02",
        "outputId": "c17b8e21-13ec-481c-b6b1-4255681ac13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Exploring GPT-2 Architecture and Parameters**\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "In this exercise, we will explore GPT-2, a state-of-the-art language model known for its ability to generate coherent and creative text. We will examine the model's architecture, count its parameters, and investigate how changing the temperature parameter affects text generation.\n",
        "\n",
        "### **Objectives**\n",
        "1. Load the GPT-2 model.\n",
        "2. Count and display the total number of parameters.\n",
        "3. Generate text with different temperatures to observe the effect on creativity and randomness.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65B2l4jbJL8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Load the GPT-2 Model**\n",
        "\n",
        "We will start by loading the smallest version of GPT-2. This version is efficient and easy to work with, making it ideal for exploring the model's functionality.\n"
      ],
      "metadata": {
        "id": "hyGqJKMVJaoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained GPT-2 model and tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "print(\"GPT-2 model loaded successfully!\")\n"
      ],
      "metadata": {
        "id": "bbCMKO0TJMn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Count the Number of Parameters**\n",
        "\n",
        "Let’s find out how many parameters the GPT-2 model contains. This information will help us understand the model's complexity and capacity.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pMLWeFTkJbxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count the total number of parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Print the result\n",
        "print(f\"Total number of parameters in GPT-2: {total_params:,}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "heza8C59Jb8M",
        "outputId": "575a71f9-e9ea-4b7f-a26f-d6eec742b1b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in GPT-2: 124,439,808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Test Text Generation with Different Temperatures**\n",
        "\n",
        "The `temperature` parameter controls the randomness of the generated text. A low temperature makes the output more deterministic, while a high temperature introduces more randomness. We’ll test different temperature values to see how they affect the text generation.\n",
        "\n"
      ],
      "metadata": {
        "id": "gw3gsyfEJcP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to generate text with temperature and attention mask\n",
        "def generate_text_with_attention_mask(temperature):\n",
        "    # Input text for the model to generate from\n",
        "    input_text = \"my hole life was \"\n",
        "\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    # Create attention mask (1s for real tokens, 0s for padding tokens)\n",
        "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long)\n",
        "\n",
        "    # Generate text with the specified temperature\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        temperature=temperature,\n",
        "        do_sample=True,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # Decode and return the generated text\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Test different temperature values\n",
        "temperatures = [0.1, 0.3, 0.6, 1.0]\n",
        "\n",
        "for temp in temperatures:\n",
        "    print(f\"\\nTemperature: {temp}\")\n",
        "    print(generate_text_with_attention_mask(temp))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xI6h3YjVJcYl",
        "outputId": "e3c75e53-742f-426c-f956-02440fc76f4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Temperature: 0.1\n",
            "my hole life was  a little bit more fun than I thought it would be. I was able to get a little more out of my life and I'm glad I did. I'm glad I did. I'm glad I did. I\n",
            "\n",
            "Temperature: 0.3\n",
            "my hole life was  in the sky, and I was a little bit scared. I was just a little bit scared.\n",
            "I was a little bit scared. I was a little bit scared. I was a little bit scared.\n",
            "I\n",
            "\n",
            "Temperature: 0.6\n",
            "my hole life was  very good! I am still very happy with my life as I am now in my 30's. I love eating and I want to do more. I am looking forward to the next month and I am looking forward to\n",
            "\n",
            "Temperature: 1.0\n",
            "my hole life was _______ on the moon, that it was not so bad until he left the land, and that he returned to earth a week after that on November 12, 1723, when his body was discovered, there was an accident of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GkPyVlhVQFTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### Explanation:\n",
        "- **`attention_mask`**: An attention mask is created with all ones because GPT-2 does not use padding tokens in its architecture. However, it's still good practice to include it.\n",
        "- **`pad_token_id`**: Set to `tokenizer.eos_token_id` to handle open-end generation properly, as GPT-2 uses EOS tokens for indicating the end of sequences.\n",
        "\n",
        " This code snippet ensures that you handle the attention mask and padding token ID correctly when generating text with GPT-2. This approach helps avoid unexpected behavior and improves the reliability of the text generation results.\n",
        "\n"
      ],
      "metadata": {
        "id": "YjcwxACfJcge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Exploring Hugging Face Pipelines**\n",
        "\n",
        "In this exercise, you will work with two Hugging Face pipelines to perform different NLP tasks. You will also have the opportunity to explore additional pipelines on your own.\n",
        "\n",
        "#### **1. Sentiment Analysis Pipeline**\n",
        "\n",
        "The sentiment analysis pipeline allows you to determine the sentiment (positive, negative, or neutral) of a given piece of text.\n",
        "\n"
      ],
      "metadata": {
        "id": "CYooBPeQVwRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1:**\n",
        "- Test the sentiment analysis pipeline with different texts, such as product reviews or social media comments.\n",
        "- Analyze how the sentiment of various texts is classified.\n",
        "# New Section"
      ],
      "metadata": {
        "id": "wsKck0TxWOGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Example text\n",
        "text = \"Hugging Face provides amazing tools for natural language processing!\"\n",
        "\n",
        "# Perform sentiment analysis\n",
        "result = sentiment_pipeline(text)\n",
        "print(\"Sentiment Analysis Result:\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiNQhtWWV2XJ",
        "outputId": "f111f579-d005-483b-8d14-424affcfe5c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment Analysis Result:\n",
            "[{'label': 'POSITIVE', 'score': 0.9998632669448853}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2:**\n",
        "\n",
        "#### **Image-to-Text (Captioning) Pipeline**\n",
        "\n",
        "The image-to-text pipeline generates captions for images. For this exercise, you will need to find and use an image-to-text pipeline from Hugging Face.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. **Search for an Image-to-Text Pipeline:**\n",
        "   - Go to the Hugging Face Model Hub.\n",
        "   - Search for an image-to-text model, such as `Salesforce/blip-image-captioning-base` or similar.\n",
        "\n",
        "2. **Set Up the Pipeline:**\n",
        "   - Use the model you found to create an image-to-text pipeline.\n",
        "\n",
        "3. **Generate Captions:**\n",
        "   - Test the pipeline by providing images and observing the generated captions."
      ],
      "metadata": {
        "id": "PB-mfPcBWJcx"
      }
    }
  ]
}